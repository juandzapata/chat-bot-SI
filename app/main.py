from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from config.settings import settings
from rag.chroma_manager import add_document
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="ChatBot IA - Universidad de Caldas",
    description="Backend con FastAPI + Docker + ChromaDB + Gemini",
    version="0.1.0",
)

# Configurar CORS para permitir requests del frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n, especifica el dominio exacto
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def read_root():
    # Conexi√≥n r√°pida para probar que Chroma funciona
    try:
        from rag.chroma_client import get_chroma_client
        client = get_chroma_client()
        client.list_collections()
        status = "Conexi√≥n exitosa a ChromaDB"
    except Exception as e:
        status = f"Error conectando a ChromaDB: {e}"

    return {
        "status": "ChatBot IA funcionando correctamente",
        "mode": settings.MODE,
        "chroma_status": status
    }

# üöÄ Nuevo endpoint de prueba
@app.post("/ingest_test")
def ingest_test():
    try:
        texto = "La Universidad de Caldas es una instituci√≥n p√∫blica ubicada en Manizales, Colombia, reconocida por su excelencia acad√©mica."
        resultado = add_document(
            collection_name="documentos_ucaldas",
            document_id="doc_test_1",
            text=texto,
            metadata={"origen": "prueba_inicial"}
        )
        return {"status": "ok", "message": resultado}
    except Exception as e:
        return {"status": "error", "message": str(e)}


# üöÄ Endpoint para ingerir todo el corpus
@app.post("/ingest_all")
def ingest_all():
    """
    Endpoint para ingerir todos los documentos del corpus en ChromaDB.
    Lee corpus_metadata.json y procesa todos los archivos.
    """
    try:
        from rag.ingest_all import ingest_all_documents
        result = ingest_all_documents()
        
        if result.get("success"):
            return {
                "status": "ok",
                "message": "Ingesta completada exitosamente",
                "summary": {
                    "total_documents": result.get("total_documents", 0),
                    "successful": result.get("successful", 0),
                    "failed": result.get("failed", 0)
                },
                "details": result.get("results", [])
            }
        else:
            raise HTTPException(status_code=500, detail=result.get("message", "Error desconocido"))
            
    except Exception as e:
        logger.error(f"Error en /ingest_all: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# üöÄ Endpoint de chat con RAG
@app.post("/chat")
def chat(query: dict):
    """
    Endpoint para realizar consultas al chatbot con RAG.
    
    Body esperado:
    {
        "question": "¬øCu√°l es la normativa sobre IA en Colombia?",
        "top_k": 3,  // opcional, n√∫mero de documentos a recuperar
        "model": "gemini",  // opcional, modelo a usar: "gemini" o "llama3"
        "mode": "extended"  // opcional, modo de respuesta: "brief" o "extended"
    }
    """
    try:
        question = query.get("question", "")
        top_k = query.get("top_k", 3)
        model_id = query.get("model", "gemini")  # Default a Gemini
        response_mode = query.get("mode", "extended")  # Default a extendido
        
        # Validar modo de respuesta
        if response_mode not in ["brief", "extended"]:
            raise HTTPException(
                status_code=400, 
                detail="El modo debe ser 'brief' o 'extended'"
            )
        
        if not question or len(question.strip()) == 0:
            raise HTTPException(status_code=400, detail="La pregunta no puede estar vac√≠a")
        
        # Importar componentes necesarios
        from rag.chroma_manager import get_or_create_collection
        from rag.models import model_manager
        
        # Obtener colecci√≥n
        collection = get_or_create_collection("documentos_ucaldas")
        
        # Buscar documentos relevantes
        logger.info(f"Buscando contexto relevante para: {question}")
        results = collection.query(
            query_texts=[question],
            n_results=top_k
        )
        
        # Construir contexto
        context_parts = []
        if results['documents'] and len(results['documents'][0]) > 0:
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                context_parts.append(f"[Documento {i+1}]: {doc}")
        
        context = "\n\n".join(context_parts) if context_parts else "No se encontr√≥ contexto relevante."
        
        # Instrucci√≥n adicional seg√∫n el modo (para Gemini)
        mode_instruction = ""
        if response_mode == "brief":
            mode_instruction = "\n\nIMPORTANTE: Proporciona una respuesta BREVE y CONCISA (m√°ximo 150 palabras)."
        else:
            mode_instruction = "\n\nIMPORTANTE: Proporciona una respuesta DETALLADA y COMPLETA (entre 400-600 palabras)."
        
        # Generar prompt seg√∫n el modelo
        if model_id == "gemini":
            prompt = f"""Eres un asistente acad√©mico de la Universidad de Caldas especializado en normativas de Inteligencia Artificial.

Bas√°ndote √öNICAMENTE en el siguiente contexto de los documentos oficiales, responde la pregunta del usuario de manera precisa y acad√©mica.{mode_instruction}

CONTEXTO:
{context}

PREGUNTA: {question}

RESPUESTA:"""
        else:  # LLaMA3
            prompt = f"""Bas√°ndote √öNICAMENTE en el siguiente contexto de los documentos oficiales, responde la pregunta del usuario de manera precisa y acad√©mica.{mode_instruction}

CONTEXTO:
{context}

PREGUNTA: {question}

RESPUESTA:"""
        
        # Generar respuesta con el modelo seleccionado
        logger.info(f"Generando respuesta con {model_id} en modo {response_mode}...")
        answer = model_manager.generate_response(prompt, model_id, response_mode)
        
        # Preparar metadatos de los documentos citados
        cited_docs = []
        if results['metadatas'] and results['metadatas'][0]:
            for i, metadata in enumerate(results['metadatas'][0]):
                cited_docs.append({
                    "title": metadata.get('titulo', 'Sin t√≠tulo'),
                    "source": metadata.get('organismo', 'Fuente desconocida'),
                    "category": metadata.get('categoria', ''),
                    "year": metadata.get('anio', 'N/A')
                })
        
        return {
            "status": "ok",
            "answer": answer,
            "question": question,
            "model_used": model_id,
            "response_mode": response_mode,
            "sources": cited_docs,
            "context_used": len(cited_docs)
        }
        
    except Exception as e:
        logger.error(f"Error en /chat: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# üîç Endpoint para ver estad√≠sticas de la colecci√≥n
@app.get("/collection_stats")
def get_collection_stats():
    """Retorna estad√≠sticas de la colecci√≥n de documentos."""
    try:
        from rag.chroma_manager import get_or_create_collection
        
        collection = get_or_create_collection("documentos_ucaldas")
        
        # Obtener conteo de documentos
        count = collection.count()
        
        return {
            "status": "ok",
            "collection": "documentos_ucaldas",
            "total_chunks": count,
            "message": f"Colecci√≥n contiene {count} chunks de documentos"
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo stats: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ü§ñ Endpoint para listar modelos disponibles
@app.get("/models")
def get_available_models():
    """Retorna lista de modelos disponibles y modos de respuesta."""
    try:
        from rag.models import model_manager
        
        models = model_manager.get_available_models()
        default_model = model_manager.get_default_model()
        
        return {
            "status": "ok",
            "available_models": models,
            "default_model": default_model,
            "total_models": len(models),
            "response_modes": [
                {
                    "id": "brief",
                    "name": "Breve",
                    "description": "Respuesta concisa (~200 tokens, ~150 palabras)",
                    "max_tokens": 200
                },
                {
                    "id": "extended",
                    "name": "Extendido",
                    "description": "Respuesta detallada (~800 tokens, ~600 palabras)",
                    "max_tokens": 800
                }
            ],
            "default_mode": "extended"
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo modelos: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# üìö Endpoint para obtener todas las fuentes disponibles
@app.get("/sources")
def get_sources():
    """
    Retorna todas las fuentes documentales disponibles en la base de datos,
    agrupadas por categor√≠a y deduplicadas por t√≠tulo.
    """
    try:
        from rag.chroma_manager import get_all_sources
        from collections import defaultdict
        
        # Obtener todas las fuentes
        all_sources = get_all_sources()
        
        # Agrupar por categor√≠a y deduplicar por t√≠tulo
        sources_by_category = defaultdict(dict)  # Cambio: dict en lugar de list
        for source in all_sources:
            category = source.get("category", "sin_categoria")
            title = source["title"]
            
            # Solo agregar si no existe (deduplicaci√≥n por t√≠tulo)
            if title not in sources_by_category[category]:
                sources_by_category[category][title] = {
                    "title": title,
                    "source": source["source"],
                    "year": source["year"]
                }
        
        # Convertir a formato de respuesta
        categories = []
        category_names = {
            "colombia": "Colombia",
            "internacional": "Internacional",
            "universidad": "Universidad de Caldas",
            "sin_categoria": "Sin Categor√≠a"
        }
        
        total_unique_sources = 0
        for category, sources_dict in sources_by_category.items():
            sources_list = list(sources_dict.values())
            total_unique_sources += len(sources_list)
            
            categories.append({
                "category": category,
                "category_name": category_names.get(category, category.capitalize()),
                "sources": sources_list,
                "count": len(sources_list)
            })
        
        # Ordenar categor√≠as (colombia, internacional, universidad, otros)
        category_order = ["colombia", "internacional", "universidad"]
        categories.sort(key=lambda x: category_order.index(x["category"]) if x["category"] in category_order else 999)
        
        return {
            "status": "ok",
            "total_sources": total_unique_sources,  # Ahora muestra documentos √∫nicos
            "total_categories": len(categories),
            "categories": categories
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo fuentes: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# üß™ Endpoint de prueba para Gemini sin RAG
@app.post("/test_gemini")
def test_gemini(query: dict):
    """Endpoint de prueba para Gemini sin contexto RAG."""
    try:
        from rag.models import model_manager
        
        question = query.get("question", "¬øQu√© es la inteligencia artificial?")
        mode = query.get("mode", "brief")
        
        # Prompt simple sin contexto
        simple_prompt = f"Responde brevemente: {question}"
        
        answer = model_manager.generate_response(simple_prompt, "gemini", mode)
        
        return {
            "status": "ok",
            "answer": answer,
            "mode": mode
        }
    except Exception as e:
        logger.error(f"Error en test_gemini: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
